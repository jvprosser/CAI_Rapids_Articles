{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f168aad1-1adf-472e-809d-8fe7c744b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings, sys, logging\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "# use the GPU-native implementation\n",
    "from spark_rapids_ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pprint\n",
    "\n",
    "# Force-clear any hanging Py4J connections\n",
    "from py4j.java_gateway import java_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9ba8cd-19bd-4080-a819-a3a7e169929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = os.environ[\"PROJECT_OWNER\"]\n",
    "DBNAME = \"DEMO_\"+USERNAME\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "STORAGE =  os.environ[\"DATA_STORAGE\"] \n",
    "DATE = date.today()\n",
    "\n",
    "RAPIDS_JAR = \"/home/cdsw/rapids-4-spark_2.12-25.10.0.jar\"\n",
    "\n",
    "\n",
    "LOCAL_PACKAGES = \"/home/cdsw/.local/lib/python3.10/site-packages\"\n",
    "# This is where the specific CUDA 12 NVRTC library lives\n",
    "NVRTC_LIB_PATH = f\"{LOCAL_PACKAGES}/nvidia/cuda_nvrtc/lib\"\n",
    "WRITABLE_CACHE_DIR = \"/tmp/cupy_cache\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad97b186-ffa2-4b5b-8654-461fcacd5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force-clear any hanging Py4J connections\n",
    "from py4j.java_gateway import java_import\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "984a95b0-b6e6-4410-9c00-24aefaa161ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to jprosser\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark-Rapids-32GB-Final\") \\\n",
    "    .config(\"spark.jars\", RAPIDS_JAR) \\\n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "    .config(\"spark.executor.resource.gpu.vendor\", \"nvidia.com\") \\\n",
    "    .config(\"spark.executor.resource.gpu.discoveryScript\", \"/home/cdsw/spark-rapids-ml/getGpusResources.sh\") \\\n",
    "    .config(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{NVRTC_LIB_PATH}:{os.environ.get('LD_LIBRARY_PATH', '')}\") \\\n",
    "    .config(\"spark.executorEnv.PYTHONPATH\", LOCAL_PACKAGES) \\\n",
    "    .config(\"spark.executorEnv.CUPY_CACHE_DIR\", WRITABLE_CACHE_DIR) \\\n",
    "    .config(\"spark.driverEnv.CUPY_CACHE_DIR\", WRITABLE_CACHE_DIR) \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", f\"-Djava.library.path={NVRTC_LIB_PATH}\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "    .config(\"spark.executor.cores\", 3) \\\n",
    "    .config(\"spark.executor.instances\", 1) \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.resource.gpu.amount\", 1) \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"10g\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"1200\") \\\n",
    "    .config(\"spark.sql.cache.serializer\", \"com.nvidia.spark.ParquetCachedBatchSerializer\") \\\n",
    "    .config('spark.sql.shuffle.partitions', '200') \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .config(\"spark.rapids.shims-provider-override\", \"com.nvidia.spark.rapids.shims.spark351.SparkShimServiceProvider\") \\\n",
    "    .config(\"spark.rapids.memory.pinnedPool.size\", \"4g\") \\\n",
    "    .config(\"spark.task.resource.gpu.amount\", 0.33) \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"s3a://go01-demo/user/jprosser/spark-rapids-ml/\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    "    .config('spark.shuffle.file.buffer', '64k') \\\n",
    "    .config('spark.shuffle.spill.compress', 'true') \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"s3a://go01-demo/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8cf6df-9fa2-4f66-b6ef-4dedf7c93527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Java Context Object: org.apache.spark.api.java.JavaSparkContext@4c0da3a'\n",
      "'Master: k8s://https://172.20.0.1:443'\n",
      "'Spark User: jprosser'\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "# View the underlying Java Spark Context\n",
    "pprint.pprint(f\"Java Context Object: {spark.sparkContext._jsc}\")\n",
    "\n",
    "# View the Spark Master (in CML, this usually points to the local container or YARN)\n",
    "pprint.pprint(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# View the User running the session\n",
    "pprint.pprint(f\"Spark User: {spark.sparkContext.sparkUser()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19fde3c3-fa2e-4bd1-8a3e-598950130e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable CollectLimit so that large datasets are collected on the GPU.\n",
    "# Not worth it for small datasets\n",
    "spark.conf.set(\"spark.rapids.sql.exec.CollectLimitExec\", \"true\")\n",
    "\n",
    "# Enabled to let the GPU to handle the random sampling of rows for large datasets\n",
    "spark.conf.set(\"spark.rapids.sql.exec.SampleExec\", \"true\")\n",
    "\n",
    "# Enabled to let allow more time for large broadcast joins\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"1200\") # Increase to 20 mins\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#spark.conf.set(\"spark.rapids.sql.explain\", \"ALL\")\n",
    "spark.conf.set(\"spark.rapids.sql.explain\", \"NOT_ON_GPU\") # Only log when/why the GPU was not selected\n",
    "spark.conf.set(\"spark.rapids.sql.variable.float.allow\", \"true\") # Allow float math\n",
    "\n",
    "# Allow the GPU to cast instead of pushing back to CPU just for cast\n",
    "spark.conf.set(\"spark.rapids.sql.castFloatToDouble.enabled\", \"true\") \n",
    "spark.conf.set(\"spark.rapids.sql.format.parquet.enabled\", \"true\")\n",
    "\n",
    "# Turning off Adaptive Query Execution (AQE) makes the entire SQL plan use the GPU\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f9c003-2869-4cee-997d-cccc74f9bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d7b7adc-c11f-439f-9420-f5d7689747f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Driver Version: 13000\n",
      "Device Count: 1\n",
      "Dynamic Allocation: false\n",
      "Executor Instances: 1\n",
      "Dynamic Allocation Enabled: false\n"
     ]
    }
   ],
   "source": [
    "# Test if the JVM can actually talk to the CUDA driver\n",
    "cuda_manager = spark._jvm.ai.rapids.cudf.Cuda\n",
    "print(f\"CUDA Driver Version: {cuda_manager.getDriverVersion()}\")\n",
    "print(f\"Device Count: {cuda_manager.getDeviceCount()}\")\n",
    "print(f\"Dynamic Allocation: {spark.conf.get('spark.dynamicAllocation.enabled')}\")\n",
    "print(f\"Executor Instances: {spark.conf.get('spark.executor.instances')}\")\n",
    "print(f\"Dynamic Allocation Enabled: {spark.conf.get('spark.dynamicAllocation.enabled')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e89282-3aed-4d6a-94a8-d4f9b455af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug enabled for RAPIDS: True\n"
     ]
    }
   ],
   "source": [
    "# Test acess to the SQLPlugin\n",
    "sql_plugin = spark._jvm.com.nvidia.spark.SQLPlugin()\n",
    "\n",
    "driver_comp = sql_plugin.driverPlugin()\n",
    "\n",
    "\n",
    "log_manager = spark._jvm.org.apache.log4j.LogManager\n",
    "level_debug = spark._jvm.org.apache.log4j.Level.DEBUG\n",
    "\n",
    "logger = driver_comp.log()\n",
    "log_manager.getLogger(\"com.nvidia.spark.rapids\").setLevel(level_debug)\n",
    "\n",
    "print(f\"Debug enabled for RAPIDS: {driver_comp.isTraceEnabled() or True}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e2175a-9033-4975-86f3-0e190d628a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/09 17:32:18 WARN  client.HiveClientImpl: [Thread-6]: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "Hive Session ID = d3e6efd9-8f61-4ad1-8d79-042c073a109f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 15\n",
      "Schema: StructType([StructField('age', FloatType(), True), StructField('credit_card_balance', FloatType(), True), StructField('bank_account_balance', FloatType(), True), StructField('mortgage_balance', FloatType(), True), StructField('sec_bank_account_balance', FloatType(), True), StructField('savings_account_balance', FloatType(), True), StructField('sec_savings_account_balance', FloatType(), True), StructField('total_est_nworth', FloatType(), True), StructField('primary_loan_balance', FloatType(), True), StructField('secondary_loan_balance', FloatType(), True), StructField('uni_loan_balance', FloatType(), True), StructField('longitude', FloatType(), True), StructField('latitude', FloatType(), True), StructField('transaction_amount', FloatType(), True), StructField('fraud_trx', IntegerType(), True)])\n",
      "== Physical Plan ==\n",
      "GpuColumnarToRow (6)\n",
      "+- GpuGlobalLimit (5)\n",
      "   +- GpuShuffleCoalesce (4)\n",
      "      +- GpuColumnarExchange (3)\n",
      "         +- GpuLocalLimit (2)\n",
      "            +- GpuScan parquet spark_catalog.default.datalaketable (1)\n",
      "\n",
      "\n",
      "(1) GpuScan parquet spark_catalog.default.datalaketable\n",
      "Output [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Batched: true\n",
      "Eager_IO_Prefetch: false\n",
      "Location: InMemoryFileIndex [s3a://go01-demo/warehouse/tablespace/external/hive/customer_data HB01/warehouse/tablespace/external/hive/datalaketable]\n",
      "ReadSchema: struct<age:float,credit_card_balance:float,bank_account_balance:float,mortgage_balance:float,sec_bank_account_balance:float,savings_account_balance:float,sec_savings_account_balance:float,total_est_nworth:float,primary_loan_balance:float,secondary_loan_balance:float,uni_loan_balance:float,longitude:float,latitude:float,transaction_amount:float,fraud_trx:int>\n",
      "\n",
      "(2) GpuLocalLimit\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 5\n",
      "\n",
      "(3) GpuColumnarExchange\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: gpusinglepartitioning$(), ENSURE_REQUIREMENTS, [plan_id=10]\n",
      "\n",
      "(4) GpuShuffleCoalesce\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 1073741824\n",
      "\n",
      "(5) GpuGlobalLimit\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 5, 0\n",
      "\n",
      "(6) GpuColumnarToRow\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do a test connection to a DB and show the GPU in action\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read.table(\"DataLakeTable\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"Schema: {df.schema}\")\n",
    "# Look for 'Gpu' operators in the output\n",
    "df.limit(5).explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9316aa9e-0b91-4cc1-9489-33e6066f5e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Experimental Methods: org.apache.spark.sql.ExperimentalMethods@7a93180d'\n"
     ]
    }
   ],
   "source": [
    "# Access the Java 'SessionState' through the back door\n",
    "jvm_session_state = spark._jsparkSession.sessionState()\n",
    "\n",
    "# Check if the Catalyst Optimizer is using the RAPIDS extensions\n",
    "pprint.pprint(f\"Experimental Methods: {jvm_session_state.experimentalMethods()}\")\n",
    "\n",
    "# Access the experimental methods via the JVM bridge\n",
    "experimental = spark._jsparkSession.sessionState().experimentalMethods()\n",
    "\n",
    "# Enable SampleExec (another commonly disabled-by-default op)\n",
    "spark.conf.set(\"spark.rapids.sql.exec.SampleExec\", \"true\")\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"1200\") # Increase to 20 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f1344f-9b70-41de-a316-e99b226e349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/09 17:32:52 WARN  scheduler.TaskSchedulerImpl: [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/09 17:33:07 WARN  scheduler.TaskSchedulerImpl: [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/09 17:33:22 WARN  scheduler.TaskSchedulerImpl: [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/09 17:33:37 WARN  scheduler.TaskSchedulerImpl: [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/09 17:33:52 WARN  scheduler.TaskSchedulerImpl: [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "26/02/09 17:34:07 WARN  scheduler.TaskSchedulerImpl: [task-starvation-timer]: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "# Create two large-ish dataframes and join them\n",
    "# This creates 10 million rows to give the GPU something to chew on\n",
    "left_df = spark.range(0, 10000000) \\\n",
    "    .withColumn(\"join_key\", F.col(\"id\") % 1000) \\\n",
    "    .withColumn(\"data_value\", F.rand(seed=42) * 100)\n",
    "\n",
    "right_df = spark.range(0, 1000) \\\n",
    "    .withColumnRenamed(\"id\", \"join_key\") \\\n",
    "    .withColumn(\"category\", F.concat(F.lit(\"Category_\"), F.col(\"join_key\")))\n",
    "\n",
    "# We use a inner join here on 'join_key'\n",
    "# GPUs prefer HASH so we give it a hint\n",
    "joined_df = left_df.hint(\"SHUFFLE_HASH\").join(right_df, on=\"join_key\", how=\"inner\")\n",
    "\n",
    "# Perform an Aggregation to trigger a Shuffle\n",
    "final_result = joined_df.groupBy(\"category\") \\\n",
    "    .agg(F.avg(\"data_value\").alias(\"avg_val\")) \\\n",
    "    .orderBy(F.desc(\"avg_val\"))\n",
    "\n",
    "\n",
    "final_result.show(10)\n",
    "\n",
    "# Check the Physical Plan\n",
    "# Look for 'GpuHashJoin' and 'GpuColumnarExchange'\n",
    "final_result.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acd08e-0a41-4352-95ea-bf6b0c2193e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data into a single vector column \n",
    "feature_cols = [\"age\", \"credit_card_balance\", \"bank_account_balance\", \"mortgage_balance\", \"sec_bank_account_balance\", \"savings_account_balance\",\n",
    "                    \"sec_savings_account_balance\", \"total_est_nworth\", \"primary_loan_balance\", \"secondary_loan_balance\", \"uni_loan_balance\",\n",
    "                    \"longitude\", \"latitude\", \"transaction_amount\"]\n",
    "\n",
    "# Avoid VectorAssembler as it creates VectorUDT data types that are not GPU Friendly\n",
    "#assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "#df_assembled = assembler.transform(df)\n",
    "# Split data into training and test sets\n",
    "#(training_data, test_data) = df_assembled.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "(training_data, test_data) = df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "\n",
    "# Use spark_rapids_ml.classification.RandomForestClassifier\n",
    "\n",
    "# Import from spark_rapids_ml to use the GPU-native implementation\n",
    "from spark_rapids_ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define the RAPIDS-native classifier\n",
    "# As noted above, by using 'featuresCols' (list of strings), we avoid VectorAssembler \n",
    "# \n",
    "rf_classifier = RandomForestClassifier(\n",
    "    labelCol=\"fraud_trx\", \n",
    "    featuresCols=feature_cols, \n",
    "    numTrees=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d1ec5-4f73-4076-8699-1f3c5076eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# This runs the training logic in C++ on the GPU via cuML\n",
    "print(\"Training Spark RAPIDS ML model...\")\n",
    "rf_model = rf_classifier.fit(training_data)\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d90bb-fcd7-441d-8240-b58706be5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and optimize the output\n",
    "# We drop 'probability' and 'rawPrediction' because they are VectorUDT types\n",
    "# that Spark SQL would otherwise force back to the CPU for formatting.\n",
    "predictions = rf_model.transform(test_data).drop(\"probability\", \"rawPrediction\")\n",
    "rf_model.setFeaturesCols(feature_cols)\n",
    "# Show results (This will be fully accelerated)\n",
    "predictions.select(\"prediction\", \"fraud_trx\").show(5)\n",
    "# Verify GPU Plan\n",
    "# You should see 'GpuProject' and 'GpuFilter' nodes without the VectorUDT warning\n",
    "predictions.explain(mode=\"formatted\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6ef8b-ec47-4323-9cac-8551a405c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark_rapids_ml.metrics.MulticlassMetrics as mm\n",
    "#print(f\"Available in metrics: {help(mm)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0eb45c-c643-45a5-ac15-b69e669b4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = predictions.filter(\"prediction = fraud_trx\").count() / predictions.count()\n",
    "\n",
    "print(f\"GPU-Accelerated Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "686b8c1c-acf4-4442-8d8f-0c5fc07df74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/08 16:35:11 WARN  rapids.GpuOverrides: [Thread-6]: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, prediction#238, true, false, true), staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, fraud_trx#337, true, false, true), staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, 1.0#338, true, false, true), StructField(prediction,DoubleType,true), StructField(fraud_trx,DoubleType,true), StructField(1.0,DoubleType,false)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    !Expression <StaticInvoke> staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, prediction#238, true, false, true) cannot run on GPU because StaticInvoke of java.lang.Double is not supported on GPU\n",
      "      @Expression <AttributeReference> prediction#238 could run on GPU\n",
      "    !Expression <StaticInvoke> staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, fraud_trx#337, true, false, true) cannot run on GPU because StaticInvoke of java.lang.Double is not supported on GPU\n",
      "      @Expression <AttributeReference> fraud_trx#337 could run on GPU\n",
      "    !Expression <StaticInvoke> staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, 1.0#338, true, false, true) cannot run on GPU because StaticInvoke of java.lang.Double is not supported on GPU\n",
      "      @Expression <AttributeReference> 1.0#338 could run on GPU\n",
      "  !Expression <AttributeReference> obj#342 cannot run on GPU because expression AttributeReference obj#342 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from spark_rapids_ml.metrics.MulticlassMetrics import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"fraud_trx\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa0567-24f6-4983-83eb-336a2222e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_rapids_ml.classification import RandomForestClassifier as cuRFC\n",
    "from spark_rapids_ml.classification import RandomForestClassificationModel as cuRFCM\n",
    "#print(f\"Available in metrics: {help(cuRFCM)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f645cd2-3f1e-41bd-ae0f-17d302504497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(rf_model)\n",
    "# save to s3a\n",
    "#rf_model.save(\"spark-rapids-ml/cuRFCM.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be12f50-8225-4941-8a10-a39ddec4c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://medium.com/rapids-ai/rapids-forest-inference-library-prediction-at-100-million-rows-per-second-19558890bc35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9f6c34a-260d-4aa4-8930-01063ca51620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential internal keys: ['_cuml_params', '_model_attributes', '_model_json', '_treelite_model', '_rf_spark_model', '_this_model']\n",
      "_model_json\n",
      "_treelite_model\n",
      "_rf_spark_model\n",
      "_cuml_params\n"
     ]
    }
   ],
   "source": [
    "# 1. Check the internal __dict__ for anything containing 'json' or 'model'\n",
    "internal_keys = [k for k in rf_model.__dict__.keys() if 'model' in k or 'json' in k or 'cuml' in k]\n",
    "print(f\"Potential internal keys: {internal_keys}\")\n",
    "\n",
    "import json\n",
    "\n",
    "# 2. Try to extract from the most likely Spark RAPIDS internal locations\n",
    "model_data = None\n",
    "\n",
    "if hasattr(rf_model, \"_model_json\"):\n",
    "    print(\"_model_json\")\n",
    "    json_model_data = rf_model._model_json\n",
    "    #pprint.pprint(json_model_data)\n",
    "\n",
    "    json_str = json.dumps(json_model_data) if isinstance(json_model_data, list) else json_model_data\n",
    "    with open(\"fraud_rf_model.json\", \"w\") as f:\n",
    "        f.write(json_str)\n",
    "if hasattr(rf_model, \"_treelite_model\"):\n",
    "    print(\"_treelite_model\") \n",
    "    treelite_model_data = rf_model._treelite_model\n",
    "    #pprint.pprint(treelite_model_data)\n",
    "    \n",
    "    with open(\"fraud_rf_model.tl\", \"w\") as f:\n",
    "        f.write(treelite_model_data)\n",
    "if \"_rf_spark_model\" in rf_model.__dict__:\n",
    "    print(\"_rf_spark_model\")    \n",
    "    model_data = rf_model.__dict__[\"_rf_spark_model\"]\n",
    "    #pprint.pprint(model_data)        \n",
    "if \"_cuml_params\" in rf_model.__dict__:\n",
    "    print(\"_cuml_params\")    \n",
    "    model_data = rf_model.__dict__[\"_cuml_params\"]\n",
    "    #pprint.pprint(model_data)\n",
    "#    json_str = json.dumps(model_data) if isinstance(model_data, list) else model_data\n",
    "#    with open(\"fraud_rf_model_cuml.json\", \"w\") as f:\n",
    "#        f.write(json_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7d5829b8-5f7e-44e5-b76f-ca34c7adf53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treelite: 4.4.1\n",
      "cuML:     25.10.00\n",
      "spark_rapids_ml:     25.10.0\n"
     ]
    }
   ],
   "source": [
    "import treelite\n",
    "import cuml\n",
    "import spark_rapids_ml\n",
    "print(f\"Treelite: {treelite.__version__}\")\n",
    "print(f\"cuML:     {cuml.__version__}\")\n",
    "print(f\"spark_rapids_ml:     {spark_rapids_ml.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "91ad88f4-f940-474a-9ca7-ca5fd0336fc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TreeliteError",
     "evalue": "[20:21:13] /project/src/serializer.cc:193: Check failed: major_ver == 3 && minor_ver == 9: Cannot load model from a different major Treelite version or a version before 3.9.0.\nCurrently running Treelite version 4.4.1\nThe model checkpoint was generated from Treelite version 1129464167.1111578202.1095844161",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTreeliteError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Deserialize the first shard to initialize the model object\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Treelite v4.x uses 'deserialize' for binary data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m main_model \u001b[38;5;241m=\u001b[39m \u001b[43mtreelite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfraud_rf_model.tl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/model.py:372\u001b[0m, in \u001b[0;36mModel.deserialize\u001b[0;34m(cls, filename)\u001b[0m\n\u001b[1;32m    370\u001b[0m handle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[1;32m    371\u001b[0m filepath \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(filename)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[0;32m--> 372\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTreeliteDeserializeModelFromFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Model(handle\u001b[38;5;241m=\u001b[39mhandle)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/core.py:74\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TreeliteError(_LIB\u001b[38;5;241m.\u001b[39mTreeliteGetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTreeliteError\u001b[0m: [20:21:13] /project/src/serializer.cc:193: Check failed: major_ver == 3 && minor_ver == 9: Cannot load model from a different major Treelite version or a version before 3.9.0.\nCurrently running Treelite version 4.4.1\nThe model checkpoint was generated from Treelite version 1129464167.1111578202.1095844161"
     ]
    }
   ],
   "source": [
    "# cuML v25.10.00 requires Treelite v4.4.1 but the serialized Treelite model it generates\n",
    "# appears to be  v3.9\n",
    "main_model = treelite.Model.deserialize(\"fraud_rf_model.tl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b95d71-d50b-4771-ab9a-f3e992abf2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuML v25.10.00 requires Treelite v4.4.1 but the serialized Treelite model it generates\n",
    "# appears to be  v3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e903800-7785-4286-a6ac-2be51ac1cda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19721c2d-87a2-4e71-a465-c393d88d54dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STAGE 2: CLEAN LOAD ---\n",
    "# Use the 'ignore unknown' flag to bypass the \"threshold_type\" error\n",
    "tl_model = treelite.frontend.from_xgboost_json(\n",
    "    \"fraud_rf_model.json\", \n",
    "    allow_unknown_field=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f899ad7-c19e-429a-8e11-fbcc3037b2ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tl_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtl_model\u001b[49m\u001b[38;5;241m.\u001b[39mset_metadata(\n\u001b[1;32m      2\u001b[0m     num_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m,              \u001b[38;5;66;03m# Schema requires feature count\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkBinaryClf\u001b[39m\u001b[38;5;124m'\u001b[39m,      \u001b[38;5;66;03m# Schema requires task definition\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     average_tree_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,    \u001b[38;5;66;03m# Schema requires aggregation type (RF=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     num_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m     num_class\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tl_model' is not defined"
     ]
    }
   ],
   "source": [
    "tl_model.set_metadata(\n",
    "    num_feature=14,              # Schema requires feature count\n",
    "    task_type='kBinaryClf',      # Schema requires task definition\n",
    "    average_tree_output=True,    # Schema requires aggregation type (RF=True)\n",
    "    num_target=1,\n",
    "    num_class=[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16eefe9-35e4-4e2c-9e41-d0bb32d5a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STAGE 3: METADATA STAMPING ---\n",
    "# We must manually define the task so the GPU knows how to handle the trees\n",
    "tl_model.set_metadata(\n",
    "    num_feature=14,\n",
    "    task_type='kBinaryClf',      # Binary classification for Fraud\n",
    "    average_tree_output=True,    # This makes it a Random Forest (not GBT)\n",
    "    num_target=1,\n",
    "    num_class=[1]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d664ed-726e-4aed-a073-63e5e4d5382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STAGE 4: GPU COMPILATION ---\n",
    "# This is the \"TensorRT-equivalent\" step for Tree models\n",
    "gpu_inference_engine = ForestInference.load_from_treelite_model(tl_model)\n",
    "\n",
    "print(\"ðŸš€ Optimization Complete. Model is live on GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddc23e-7bf7-4c29-8717-8b4a74cd5d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb8668-e0cc-4028-bc7c-ec7f73270852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98bf3496-75b5-4ffa-94e1-b387dcd32176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"threshold_type\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"leaf_output_type\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"num_feature\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"task_type\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"average_tree_output\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"num_target\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"num_class\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"leaf_vector_shape\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"target_id\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"class_id\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"postprocessor\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"sigmoid_alpha\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"ratio_c\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"base_scores\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"attributes\"\n",
      "  warnings.warn(py_str(msg))\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/treelite/core.py:25: UserWarning: [17:53:21] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:155: Warning: Encountered unknown key \"trees\"\n",
      "  warnings.warn(py_str(msg))\n"
     ]
    },
    {
     "ename": "TreeliteError",
     "evalue": "[17:53:21] /project/src/model_builder/model_builder.cc:281: Check failed: metadata_initialized_: The model does not yet have a valid metadata. Please add metadata by calling InitializeMetadata().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTreeliteError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1. Load the trees (ignoring the \"rich\" Spark headers)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tl_model \u001b[38;5;241m=\u001b[39m \u001b[43mtreelite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrontend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_xgboost_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfraud_rf_model.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unknown_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2. Manually initialize the missing metadata\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# For your Random Forest Classifier:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# - num_feature: 14 (as seen in your metadata)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# - task_type: 'kBinaryClf' (since it's a fraud classifier)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# - num_class: [1] (for binary/single-target models, num_class is often [1])\u001b[39;00m\n\u001b[1;32m     16\u001b[0m tl_model\u001b[38;5;241m.\u001b[39mset_metadata(\n\u001b[1;32m     17\u001b[0m     num_feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m,\n\u001b[1;32m     18\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkBinaryClf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     num_class\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/frontend.py:115\u001b[0m, in \u001b[0;36mload_xgboost_model\u001b[0;34m(filename, format_choice, allow_unknown_field)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_as_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# File name not ending with .json will be parsed as UBJSON.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_as_ubjson()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/frontend.py:99\u001b[0m, in \u001b[0;36mload_xgboost_model.<locals>.parse_as_json\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_as_json\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(\n\u001b[0;32m---> 99\u001b[0m         handle\u001b[38;5;241m=\u001b[39m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_xgboost_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unknown_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unknown_field\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/compat.py:44\u001b[0m, in \u001b[0;36mload_xgboost_model\u001b[0;34m(filename, allow_unknown_field)\u001b[0m\n\u001b[1;32m     42\u001b[0m parser_config_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(parser_config)\n\u001b[1;32m     43\u001b[0m handle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[0;32m---> 44\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTreeliteLoadXGBoostModelJSON\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_config_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/core.py:74\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TreeliteError(_LIB\u001b[38;5;241m.\u001b[39mTreeliteGetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTreeliteError\u001b[0m: [17:53:21] /project/src/model_builder/model_builder.cc:281: Check failed: metadata_initialized_: The model does not yet have a valid metadata. Please add metadata by calling InitializeMetadata()."
     ]
    }
   ],
   "source": [
    "import treelite\n",
    "from cuml.fil import ForestInference\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the trees (ignoring the \"rich\" Spark headers)\n",
    "tl_model = treelite.frontend.load_xgboost_model(\n",
    "    \"fraud_rf_model.json\", \n",
    "    allow_unknown_field=True\n",
    ")\n",
    "\n",
    "# 2. Manually initialize the missing metadata\n",
    "# For your Random Forest Classifier:\n",
    "# - num_feature: 14 (as seen in your metadata)\n",
    "# - task_type: 'kBinaryClf' (since it's a fraud classifier)\n",
    "# - num_class: [1] (for binary/single-target models, num_class is often [1])\n",
    "tl_model.set_metadata(\n",
    "    num_feature=14,\n",
    "    task_type='kBinaryClf',\n",
    "    average_tree_output=True,  # Crucial for Random Forests\n",
    "    num_target=1,\n",
    "    num_class=[1]\n",
    ")\n",
    "\n",
    "# 3. Load the model into the GPU FIL engine\n",
    "# This is the \"TensorRT-style\" bare-metal optimization step\n",
    "gpu_model = ForestInference.load_from_treelite_model(tl_model)\n",
    "\n",
    "# 4. Verify with your 14 features\n",
    "dummy_input = np.random.rand(1, 14).astype(np.float32)\n",
    "preds = gpu_model.predict(dummy_input)\n",
    "\n",
    "print(\"âœ… Model successfully optimized and loaded into GPU!\")\n",
    "print(f\"Sample Prediction: {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b53c1505-d5dd-4279-b1ab-afb2803a51f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:40:18] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:160: Error: key \"threshold_type\" is not recognized!\n"
     ]
    },
    {
     "ename": "TreeliteError",
     "evalue": "[17:40:18] /project/src/model_loader/xgboost_json.cc:135: Provided JSON could not be parsed as XGBoost model. Parsing error at offset 22: Terminate parsing due to Handler error.\n{\n    \"threshold_type\": \"float32\",\n    \"leaf_output_type\": \"float32\",\n    \"num_feature\": 14,\n    \"ta\n~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTreeliteError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load into the optimized GPU engine\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This is the \"TensorRT\" equivalent for Tree models\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m gpu_model \u001b[38;5;241m=\u001b[39m \u001b[43mForestInference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfraud_rf_model.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxgboost_json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cuml/fil/fil.pyx:807\u001b[0m, in \u001b[0;36mcuml.fil.fil.ForestInference.load\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/frontend.py:133\u001b[0m, in \u001b[0;36mload_xgboost_model\u001b[0;34m(filename, format_choice, allow_unknown_field)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_as_ubjson()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_as_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown format_choice argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_choice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/frontend.py:99\u001b[0m, in \u001b[0;36mload_xgboost_model.<locals>.parse_as_json\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_as_json\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(\n\u001b[0;32m---> 99\u001b[0m         handle\u001b[38;5;241m=\u001b[39m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_xgboost_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unknown_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unknown_field\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/compat.py:44\u001b[0m, in \u001b[0;36mload_xgboost_model\u001b[0;34m(filename, allow_unknown_field)\u001b[0m\n\u001b[1;32m     42\u001b[0m parser_config_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(parser_config)\n\u001b[1;32m     43\u001b[0m handle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[0;32m---> 44\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTreeliteLoadXGBoostModelJSON\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_config_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/core.py:74\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TreeliteError(_LIB\u001b[38;5;241m.\u001b[39mTreeliteGetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTreeliteError\u001b[0m: [17:40:18] /project/src/model_loader/xgboost_json.cc:135: Provided JSON could not be parsed as XGBoost model. Parsing error at offset 22: Terminate parsing due to Handler error.\n{\n    \"threshold_type\": \"float32\",\n    \"leaf_output_type\": \"float32\",\n    \"num_feature\": 14,\n    \"ta\n~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
     ]
    }
   ],
   "source": [
    "from cuml.fil import ForestInference\n",
    "import numpy as np\n",
    "\n",
    "# Load into the optimized GPU engine\n",
    "# This is the \"TensorRT\" equivalent for Tree models\n",
    "gpu_model = ForestInference.load(\n",
    "    \"fraud_rf_model.json\", \n",
    "    model_type=\"xgboost_json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64064bf5-760b-4b8a-9071-87ceaef1a390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully imported from cuml.fil\n"
     ]
    }
   ],
   "source": [
    "import treelite\n",
    "\n",
    "from cuml.fil import ForestInference\n",
    "print(\"âœ… Successfully imported from cuml.fil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3800e956-5909-4861-8cec-2a056df827fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "treelite_model should be either treelite.Model or bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForestInference\n\u001b[0;32m----> 6\u001b[0m gpu_inference_model \u001b[38;5;241m=\u001b[39m \u001b[43mForestInference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_treelite_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtreelite_model_data\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cuml/fil/fil.pyx:988\u001b[0m, in \u001b[0;36mcuml.fil.fil.ForestInference.load_from_treelite_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cuml/fil/fil.pyx:643\u001b[0m, in \u001b[0;36mcuml.fil.fil.ForestInference.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cuml/fil/fil.pyx:670\u001b[0m, in \u001b[0;36mcuml.fil.fil.ForestInference._load_to_fil\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: treelite_model should be either treelite.Model or bytes"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from cuml.fil import ForestInference\n",
    "\n",
    "\n",
    "\n",
    "gpu_inference_model = ForestInference.load_from_treelite_model(\n",
    "    treelite_model_data\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe0732-9c0f-48e2-8f4b-77703d7d36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpu_inference_model = ForestInference.load(treelite_model_data, model_type=\"treelite_checkpoint\")\n",
    "\n",
    "gpu_inference_model = ForestInference.load(\"fraud_rf_model.tl\", model_type=\"treelite_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db9e59b0-b2f7-48b4-8a7b-72d80bb16a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Underlying cuML Parameters ---\n",
      "{\n",
      "    \"n_streams\": 1,\n",
      "    \"n_estimators\": 20,\n",
      "    \"max_depth\": 5,\n",
      "    \"max_features\": \"auto\",\n",
      "    \"n_bins\": 32,\n",
      "    \"bootstrap\": true,\n",
      "    \"verbose\": false,\n",
      "    \"min_samples_leaf\": 1,\n",
      "    \"min_samples_split\": 2,\n",
      "    \"max_samples\": 1.0,\n",
      "    \"max_leaves\": -1,\n",
      "    \"min_impurity_decrease\": 0.0,\n",
      "    \"random_state\": 503554838,\n",
      "    \"max_batch_size\": 4096,\n",
      "    \"split_criterion\": \"gini\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your RandomForestClassificationModel instance\n",
    "import json\n",
    "params = rf_model.cuml_params\n",
    "\n",
    "print(\"--- Underlying cuML Parameters ---\")\n",
    "print(json.dumps(params, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0193917-b0a6-4f52-a5da-23ed612f0c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Could not find 'model_json' in params. Printing all keys to debug:\n",
      "['rawPredictionCol', 'probabilityCol', 'seed', 'maxDepth', 'maxBins', 'minInstancesPerNode', 'minInfoGain', 'maxMemoryInMB', 'cacheNodeIds', 'checkpointInterval', 'impurity', 'numTrees', 'featureSubsetStrategy', 'subsamplingRate', 'leafCol', 'minWeightFractionPerNode', 'bootstrap', 'labelCol', 'featuresCol', 'predictionCol', 'verbose', 'n_streams', 'min_samples_split', 'max_samples', 'max_leaves', 'min_impurity_decrease', 'max_batch_size', 'featuresCols']\n"
     ]
    }
   ],
   "source": [
    "##SKIP\n",
    "\n",
    "import json\n",
    "\n",
    "# Extract the parameter map which stores the 'model_json' from the constructor\n",
    "params = rf_model.extractParamMap()\n",
    "\n",
    "# Search the params for the model data\n",
    "model_data = None\n",
    "for p, v in params.items():\n",
    "    if \"model_json\" in p.name:\n",
    "        model_data = v\n",
    "        print(\"found it!\")\n",
    "        break\n",
    "\n",
    "if model_data:\n",
    "    # If it's a list, we need to join it into a single string\n",
    "    final_json = model_data if isinstance(model_data, str) else json.dumps(model_data)\n",
    "    \n",
    "    with open(\"fraud_rf_model.json\", \"w\") as f:\n",
    "        f.write(final_json)\n",
    "    print(\"âœ… Success! Model extracted to fraud_rf_model.json\")\n",
    "else:\n",
    "    print(\"âŒ Could not find 'model_json' in params. Printing all keys to debug:\")\n",
    "    print([p.name for p in params.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642841e9-f8f2-459d-b083-98fc8aa27060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cuml.fil import ForestInference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fde7fddd-d6ac-4bc4-8ece-6935f6b115be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:58:16] /project/src/model_loader/detail/xgboost_json/delegated_handler.cc:160: Error: key \"threshold_type\" is not recognized!\n"
     ]
    },
    {
     "ename": "TreeliteError",
     "evalue": "[17:58:16] /project/src/model_loader/xgboost_json.cc:135: Provided JSON could not be parsed as XGBoost model. Parsing error at offset 22: Terminate parsing due to Handler error.\n{\n    \"threshold_type\": \"float32\",\n    \"leaf_output_type\": \"float32\",\n    \"num_feature\": 14,\n    \"ta\n~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTreeliteError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the model into the FIL engine\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This is the 'compilation' step that gives you TensorRT-like performance\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m fm \u001b[38;5;241m=\u001b[39m \u001b[43mForestInference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfraud_rf_model.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxgboost_json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Prepare your 14 features as a float32 array\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# This will run significantly faster than Spark's .transform()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m test_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m14\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cuml/fil/fil.pyx:807\u001b[0m, in \u001b[0;36mcuml.fil.fil.ForestInference.load\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/frontend.py:133\u001b[0m, in \u001b[0;36mload_xgboost_model\u001b[0;34m(filename, format_choice, allow_unknown_field)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_as_ubjson()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_as_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown format_choice argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_choice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/frontend.py:99\u001b[0m, in \u001b[0;36mload_xgboost_model.<locals>.parse_as_json\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_as_json\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(\n\u001b[0;32m---> 99\u001b[0m         handle\u001b[38;5;241m=\u001b[39m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_xgboost_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unknown_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unknown_field\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/compat.py:44\u001b[0m, in \u001b[0;36mload_xgboost_model\u001b[0;34m(filename, allow_unknown_field)\u001b[0m\n\u001b[1;32m     42\u001b[0m parser_config_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(parser_config)\n\u001b[1;32m     43\u001b[0m handle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[0;32m---> 44\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTreeliteLoadXGBoostModelJSON\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_config_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/treelite/core.py:74\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TreeliteError(_LIB\u001b[38;5;241m.\u001b[39mTreeliteGetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTreeliteError\u001b[0m: [17:58:16] /project/src/model_loader/xgboost_json.cc:135: Provided JSON could not be parsed as XGBoost model. Parsing error at offset 22: Terminate parsing due to Handler error.\n{\n    \"threshold_type\": \"float32\",\n    \"leaf_output_type\": \"float32\",\n    \"num_feature\": 14,\n    \"ta\n~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
     ]
    }
   ],
   "source": [
    "from cuml.fil import ForestInference\n",
    "import numpy as np\n",
    "\n",
    "# Load the model into the FIL engine\n",
    "# This is the 'compilation' step that gives you TensorRT-like performance\n",
    "fm = ForestInference.load(\"fraud_rf_model.json\", model_type=\"xgboost_json\")\n",
    "\n",
    "# Prepare your 14 features as a float32 array\n",
    "# This will run significantly faster than Spark's .transform()\n",
    "test_input = np.random.rand(1, 14).astype(np.float32)\n",
    "predictions = fm.predict(test_input)\n",
    "\n",
    "print(f\"Prediction: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "605ca3e1-7009-4c22-af36-c6c8035c954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "rf_model.getNumTrees() None\n",
      "Param(parent='RandomForestClassificationModel_e5eab90dc08e', name='numTrees', doc='Number of trees to train (>= 1).')\n",
      "rf_model.numTrees None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 17:30:45 WARN  rapids.GpuOverrides: [Thread-6]:                        \n",
      "      ! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
      "        @Expression <AttributeReference> treeID#350 could run on GPU\n",
      "        @Expression <AttributeReference> metadata#351 could run on GPU\n",
      "        @Expression <AttributeReference> weights#352 could run on GPU\n",
      "\n",
      "26/02/06 17:30:47 WARN  rapids.GpuOverrides: [Thread-6]:                        \n",
      "      ! <SerializeFromObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.SerializeFromObjectExec\n",
      "        @Expression <Alias> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).treeID AS treeID#362 could run on GPU\n",
      "          ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).treeID cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "            !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "              ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "        @Expression <Alias> if (isnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData)) null else named_struct(id, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).id, prediction, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).prediction, impurity, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).impurity, impurityStats, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).impurityStats, true, false, true), rawCount, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).rawCount, gain, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).gain, leftChild, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).leftChild, rightChild, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).rightChild, split, if (isnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split)) null else named_struct(featureIndex, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).featureIndex, leftCategoriesOrThreshold, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).leftCategoriesOrThreshold, true, false, true), numCategories, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).numCategories)) AS nodeData#363 could run on GPU\n",
      "          @Expression <If> if (isnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData)) null else named_struct(id, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).id, prediction, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).prediction, impurity, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).impurity, impurityStats, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).impurityStats, true, false, true), rawCount, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).rawCount, gain, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).gain, leftChild, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).leftChild, rightChild, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).rightChild, split, if (isnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split)) null else named_struct(featureIndex, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).featureIndex, leftCategoriesOrThreshold, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).leftCategoriesOrThreshold, true, false, true), numCategories, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).numCategories)) could run on GPU\n",
      "            !Expression <IsNull> isnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "              ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                  ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                    !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "            @Expression <Literal> null could run on GPU\n",
      "            @Expression <CreateNamedStruct> named_struct(id, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).id, prediction, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).prediction, impurity, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).impurity, impurityStats, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).impurityStats, true, false, true), rawCount, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).rawCount, gain, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).gain, leftChild, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).leftChild, rightChild, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).rightChild, split, if (isnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split)) null else named_struct(featureIndex, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).featureIndex, leftCategoriesOrThreshold, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).leftCategoriesOrThreshold, true, false, true), numCategories, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).numCategories)) could run on GPU\n",
      "              @Expression <Literal> id could run on GPU\n",
      "              ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).id cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                  ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                      ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                        !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "              @Expression <Literal> prediction could run on GPU\n",
      "              ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).prediction cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                  ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                      ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                        !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "              @Expression <Literal> impurity could run on GPU\n",
      "              ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).impurity cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                  ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                      ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                        !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "              @Expression <Literal> impurityStats could run on GPU\n",
      "              !Expression <StaticInvoke> staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).impurityStats, true, false, true) cannot run on GPU because StaticInvoke of org.apache.spark.sql.catalyst.expressions.UnsafeArrayData is not supported on GPU\n",
      "                ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).impurityStats cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                  !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                    ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                      !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                        ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                          !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "              @Expression <Literal> rawCount could run on GPU\n",
      "              ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).rawCount cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                  ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                      ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                        !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "              @Expression <Literal> gain could run on GPU\n",
      "              ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).gain cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                  ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                      ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                        !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "              @Expression <Literal> leftChild could run on GPU\n",
      "              ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).leftChild cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                  ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                      ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                        !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "              @Expression <Literal> rightChild could run on GPU\n",
      "              ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).rightChild cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                  ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                      ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                        !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "              @Expression <Literal> split could run on GPU\n",
      "              @Expression <If> if (isnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split)) null else named_struct(featureIndex, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).featureIndex, leftCategoriesOrThreshold, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).leftCategoriesOrThreshold, true, false, true), numCategories, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).numCategories) could run on GPU\n",
      "                !Expression <IsNull> isnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split) cannot run on GPU because input expression Invoke knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$SplitData) is not supported)\n",
      "                  ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                            !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                @Expression <Literal> null could run on GPU\n",
      "                @Expression <CreateNamedStruct> named_struct(featureIndex, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).featureIndex, leftCategoriesOrThreshold, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).leftCategoriesOrThreshold, true, false, true), numCategories, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).numCategories) could run on GPU\n",
      "                  @Expression <Literal> featureIndex could run on GPU\n",
      "                  ! <Invoke> knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).featureIndex cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split) cannot run on GPU because input expression Invoke knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$SplitData) is not supported); expression KnownNotNull knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$SplitData)\n",
      "                      ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                        !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                          ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                            !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                              ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                                !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                  @Expression <Literal> leftCategoriesOrThreshold could run on GPU\n",
      "                  !Expression <StaticInvoke> staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).leftCategoriesOrThreshold, true, false, true) cannot run on GPU because StaticInvoke of org.apache.spark.sql.catalyst.expressions.UnsafeArrayData is not supported on GPU\n",
      "                    ! <Invoke> knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).leftCategoriesOrThreshold cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                      !Expression <KnownNotNull> knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split) cannot run on GPU because input expression Invoke knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$SplitData) is not supported); expression KnownNotNull knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$SplitData)\n",
      "                        ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                          !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                            ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                              !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                                ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                                  !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                  @Expression <Literal> numCategories could run on GPU\n",
      "                  ! <Invoke> knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split).numCategories cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                    !Expression <KnownNotNull> knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split) cannot run on GPU because input expression Invoke knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$SplitData) is not supported); expression KnownNotNull knownnotnull(knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$SplitData)\n",
      "                      ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData).split cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                        !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) cannot run on GPU because expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData); input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData (ObjectType(class org.apache.spark.ml.tree.DecisionTreeModelReadWrite$NodeData) is not supported)\n",
      "                          ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])).nodeData cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "                            !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) cannot run on GPU because input expression AssertNotNull assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) (ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData) is not supported); expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true])) produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "                              ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
      "                                !Expression <BoundReference> input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData, true] produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "        ! <ExternalRDDScanExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.ExternalRDDScanExec\n",
      "          !Expression <AttributeReference> obj#361 cannot run on GPU because expression AttributeReference obj#361 produces an unsupported type ObjectType(class org.apache.spark.ml.tree.EnsembleModelReadWrite$EnsembleNodeData)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import treelite\n",
    "\n",
    "# 1. Load the model from the JSON you just saved\n",
    "model = treelite.Model.load(\"fraud_rf_model.json\", model_format='xgboost_json')\n",
    "\n",
    "# 2. Compile the model into a shared library (.so)\n",
    "# This performs optimizations equivalent to TensorRT's layer fusion\n",
    "model.export_lib(\n",
    "    toolchain='gcc', \n",
    "    libpath='./fraud_rf_engine.so', \n",
    "    params={'parallel_comp': 4},\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3b6fa-ab80-4092-8970-0d38182549e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f3b18-1b4d-4614-a2b3-e180c1550f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
