{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a7ddba-60d9-42cd-8776-fc9da4232725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to jprosser\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Java Context Object: org.apache.spark.api.java.JavaSparkContext@30440c9f'\n",
      "'Master: k8s://https://172.20.0.1:443'\n",
      "'Spark User: jprosser'\n",
      "CUDA Driver Version: 13000\n",
      "Device Count: 1\n",
      "Dynamic Allocation: false\n",
      "Executor Instances: 1\n",
      "Dynamic Allocation Enabled: false\n",
      "Debug enabled for RAPIDS: True\n"
     ]
    }
   ],
   "source": [
    "import os, warnings, sys, logging \n",
    "import mlflow \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datetime import date\n",
    "\n",
    "#use the GPU-native implementation\n",
    "from spark_rapids_ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pprint\n",
    "\n",
    "from py4j.java_gateway import java_import\n",
    "\n",
    "USERNAME = os.environ[\"PROJECT_OWNER\"] \n",
    "DBNAME = \"DEMO_\"+USERNAME \n",
    "CONNECTION_NAME = \"go01-aw-dl\" \n",
    "STORAGE = os.environ[\"DATA_STORAGE\"] \n",
    "DATE = date.today()\n",
    "\n",
    "RAPIDS_JAR = \"/home/cdsw/rapids-4-spark_2.12-25.10.0.jar\"\n",
    "\n",
    "LOCAL_PACKAGES = \"/home/cdsw/.local/lib/python3.10/site-packages\"\n",
    "\n",
    "#This is where the specific CUDA 12 NVRTC library lives\n",
    "NVRTC_LIB_PATH = f\"{LOCAL_PACKAGES}/nvidia/cuda_nvrtc/lib\" \n",
    "WRITABLE_CACHE_DIR = \"/tmp/cupy_cache\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"Spark-Rapids-32GB-Final\") \\\n",
    ".config(\"spark.jars\", RAPIDS_JAR) \\\n",
    ".config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    ".config(\"spark.task.resource.gpu.amount\", 0.5) \\\n",
    ".config(\"spark.executor.resource.gpu.vendor\", \"nvidia.com\") \\\n",
    ".config(\"spark.executor.resource.gpu.discoveryScript\", \"/home/cdsw/spark-rapids-ml/getGpusResources.sh\") \\\n",
    ".config(\"spark.executorEnv.LD_LIBRARY_PATH\", f\"{NVRTC_LIB_PATH}:{os.environ.get('LD_LIBRARY_PATH', '')}\") \\\n",
    ".config(\"spark.executorEnv.PYTHONPATH\", LOCAL_PACKAGES) \\\n",
    ".config(\"spark.executorEnv.CUPY_CACHE_DIR\", WRITABLE_CACHE_DIR) \\\n",
    ".config(\"spark.driverEnv.CUPY_CACHE_DIR\", WRITABLE_CACHE_DIR) \\\n",
    ".config(\"spark.driver.memory\", \"12g\") \\\n",
    ".config(\"spark.driver.extraJavaOptions\", f\"-Djava.library.path={NVRTC_LIB_PATH}\") \\\n",
    ".config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    ".config(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    ".config(\"spark.executor.cores\", 2) \\\n",
    ".config(\"spark.executor.instances\", 1) \\\n",
    ".config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    ".config(\"spark.executor.memory\", \"10g\") \\\n",
    ".config(\"spark.executor.resource.gpu.amount\", 1) \\\n",
    ".config(\"spark.executor.memoryOverhead\", \"10g\") \\\n",
    ".config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    ".config(\"spark.sql.broadcastTimeout\", \"1200\") \\\n",
    ".config(\"spark.sql.cache.serializer\", \"com.nvidia.spark.ParquetCachedBatchSerializer\") \\\n",
    ".config('spark.sql.shuffle.partitions', '200') \\\n",
    ".config(\"spark.network.timeout\", \"800s\") \\\n",
    ".config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    ".config(\"spark.rapids.shims-provider-override\", \"com.nvidia.spark.rapids.shims.spark351.SparkShimServiceProvider\") \\\n",
    ".config(\"spark.rapids.memory.pinnedPool.size\", \"4g\") \\\n",
    ".config(\"spark.kerberos.access.hadoopFileSystems\", \"s3a://go01-demo/user/jprosser/spark-rapids-ml/\") \\\n",
    ".config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    ".config('spark.shuffle.file.buffer', '64k') \\\n",
    ".config('spark.shuffle.spill.compress', 'true') \\\n",
    ".config(\"spark.hadoop.fs.defaultFS\", \"s3a://go01-demo/\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "#View the underlying Java Spark Context\n",
    "pprint.pprint(f\"Java Context Object: {spark.sparkContext._jsc}\")\n",
    "\n",
    "#View the Spark Master (in CML, this usually points to the local container or YARN)\n",
    "pprint.pprint(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "#View the User running the session\n",
    "pprint.pprint(f\"Spark User: {spark.sparkContext.sparkUser()}\")\n",
    "\n",
    "#Enable CollectLimit so that large datasets are collected on the GPU.\n",
    "#Not worth it for small datasets\n",
    "spark.conf.set(\"spark.rapids.sql.exec.CollectLimitExec\", \"true\")\n",
    "\n",
    "#Enabled to let the GPU to handle the random sampling of rows for large datasets\n",
    "spark.conf.set(\"spark.rapids.sql.exec.SampleExec\", \"true\")\n",
    "\n",
    "#Enabled to let allow more time for large broadcast joins\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\", \"1200\") # Increase to 20 mins from pyspark.sql import functions as F\n",
    "\n",
    "#spark.conf.set(\"spark.rapids.sql.explain\", \"ALL\") \n",
    "spark.conf.set(\"spark.rapids.sql.explain\", \"NOT_ON_GPU\") # Only log when/why the GPU was not selected \n",
    "spark.conf.set(\"spark.rapids.sql.variable.float.allow\", \"true\") # Allow float math\n",
    "\n",
    "#Allow the GPU to cast instead of pushing back to CPU just for cast\n",
    "spark.conf.set(\"spark.rapids.sql.castFloatToDouble.enabled\", \"true\") \n",
    "spark.conf.set(\"spark.rapids.sql.format.parquet.enabled\", \"true\")\n",
    "\n",
    "#Turning off Adaptive Query Execution (AQE) makes the entire SQL plan use the GPU\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10000)\n",
    "\n",
    "#Test if the JVM can actually talk to the CUDA driver\n",
    "cuda_manager = spark._jvm.ai.rapids.cudf.Cuda \n",
    "print(f\"CUDA Driver Version: {cuda_manager.getDriverVersion()}\") \n",
    "print(f\"Device Count: {cuda_manager.getDeviceCount()}\") \n",
    "print(f\"Dynamic Allocation: {spark.conf.get('spark.dynamicAllocation.enabled')}\") \n",
    "print(f\"Executor Instances: {spark.conf.get('spark.executor.instances')}\") \n",
    "print(f\"Dynamic Allocation Enabled: {spark.conf.get('spark.dynamicAllocation.enabled')}\")\n",
    "\n",
    "#Test acess to the SQLPlugin\n",
    "sql_plugin = spark._jvm.com.nvidia.spark.SQLPlugin()\n",
    "\n",
    "driver_comp = sql_plugin.driverPlugin()\n",
    "\n",
    "log_manager = spark._jvm.org.apache.log4j.LogManager \n",
    "level_debug = spark._jvm.org.apache.log4j.Level.DEBUG\n",
    "\n",
    "logger = driver_comp.log() \n",
    "log_manager.getLogger(\"com.nvidia.spark.rapids\").setLevel(level_debug)\n",
    "\n",
    "print(f\"Debug enabled for RAPIDS: {driver_comp.isTraceEnabled() or True}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece2b290-33de-4bb3-bb24-8f84b6860dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 18:54:45 WARN  client.HiveClientImpl: [Thread-6]: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "Hive Session ID = bdfd90a7-4a16-416d-92c8-d747b15f423c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: 15\n",
      "Schema: StructType([StructField('age', FloatType(), True), StructField('credit_card_balance', FloatType(), True), StructField('bank_account_balance', FloatType(), True), StructField('mortgage_balance', FloatType(), True), StructField('sec_bank_account_balance', FloatType(), True), StructField('savings_account_balance', FloatType(), True), StructField('sec_savings_account_balance', FloatType(), True), StructField('total_est_nworth', FloatType(), True), StructField('primary_loan_balance', FloatType(), True), StructField('secondary_loan_balance', FloatType(), True), StructField('uni_loan_balance', FloatType(), True), StructField('longitude', FloatType(), True), StructField('latitude', FloatType(), True), StructField('transaction_amount', FloatType(), True), StructField('fraud_trx', IntegerType(), True)])\n",
      "== Physical Plan ==\n",
      "GpuColumnarToRow (6)\n",
      "+- GpuGlobalLimit (5)\n",
      "   +- GpuShuffleCoalesce (4)\n",
      "      +- GpuColumnarExchange (3)\n",
      "         +- GpuLocalLimit (2)\n",
      "            +- GpuScan parquet spark_catalog.default.datalaketable (1)\n",
      "\n",
      "\n",
      "(1) GpuScan parquet spark_catalog.default.datalaketable\n",
      "Output [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Batched: true\n",
      "Eager_IO_Prefetch: false\n",
      "Location: InMemoryFileIndex [s3a://go01-demo/warehouse/tablespace/external/hive/customer_data HB01/warehouse/tablespace/external/hive/datalaketable]\n",
      "ReadSchema: struct<age:float,credit_card_balance:float,bank_account_balance:float,mortgage_balance:float,sec_bank_account_balance:float,savings_account_balance:float,sec_savings_account_balance:float,total_est_nworth:float,primary_loan_balance:float,secondary_loan_balance:float,uni_loan_balance:float,longitude:float,latitude:float,transaction_amount:float,fraud_trx:int>\n",
      "\n",
      "(2) GpuLocalLimit\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 5\n",
      "\n",
      "(3) GpuColumnarExchange\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: gpusinglepartitioning$(), ENSURE_REQUIREMENTS, [plan_id=10]\n",
      "\n",
      "(4) GpuShuffleCoalesce\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 1073741824\n",
      "\n",
      "(5) GpuGlobalLimit\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 5, 0\n",
      "\n",
      "(6) GpuColumnarToRow\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"DataLakeTable\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"Schema: {df.schema}\")\n",
    "# Look for 'Gpu' operators in the output\n",
    "df.limit(5).explain(mode=\"formatted\")\n",
    "\n",
    "# Transform data into a single vector column \n",
    "feature_cols = [\"age\", \"credit_card_balance\", \"bank_account_balance\", \"mortgage_balance\", \"sec_bank_account_balance\", \"savings_account_balance\",\n",
    "                    \"sec_savings_account_balance\", \"total_est_nworth\", \"primary_loan_balance\", \"secondary_loan_balance\", \"uni_loan_balance\",\n",
    "                    \"longitude\", \"latitude\", \"transaction_amount\"]\n",
    "\n",
    "# Avoid VectorAssembler as it creates VectorUDT data types that are not GPU Friendly\n",
    "#assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "#df_assembled = assembler.transform(df)\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "#(training_data, test_data) = df_assembled.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70fd10c-a05d-4218-81f3-566cd49ac323",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "#    X, y, train_size=train_size)\n",
    "\n",
    "# Use spark_rapids_ml.classification.RandomForestClassifier\n",
    "\n",
    "# Import from spark_rapids_ml to use the GPU-native implementation\n",
    "from spark_rapids_ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define the RAPIDS-native classifier\n",
    "# As noted above, by using 'featuresCols' (list of strings), we avoid VectorAssembler \n",
    "# \n",
    "rf_classifier = RandomForestClassifier(\n",
    "    labelCol=\"fraud_trx\", \n",
    "    featuresCols=feature_cols, \n",
    "    numTrees=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c467464f-2795-4be1-8763-e917d1122513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 18:55:02,955 - spark_rapids_ml.classification.RandomForestClassifier - INFO - Training spark-rapids-ml with 1 worker(s) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Spark RAPIDS ML model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 18:55:04,084 - spark_rapids_ml.classification.RandomForestClassifier - INFO - Training tasks require the resource(cores=2, gpu=1.0)\n",
      "26/02/13 18:55:04 WARN  scheduler.DAGScheduler: [dag-scheduler-event-loop]: Barrier stage in job 0 requires 1 slots, but only 0 are available. Will retry up to 40 more times\n",
      "26/02/13 18:55:19 WARN  scheduler.DAGScheduler: [dag-scheduler-event-loop]: Barrier stage in job 0 requires 1 slots, but only 0 are available. Will retry up to 39 more times\n",
      "26/02/13 18:55:34 WARN  scheduler.DAGScheduler: [dag-scheduler-event-loop]: Barrier stage in job 0 requires 1 slots, but only 0 are available. Will retry up to 38 more times\n",
      "26/02/13 18:55:49 WARN  scheduler.DAGScheduler: [dag-scheduler-event-loop]: Barrier stage in job 0 requires 1 slots, but only 0 are available. Will retry up to 37 more times\n",
      "2026-02-13 18:57:08,992 - spark_rapids_ml.classification.RandomForestClassifier - INFO - Finished training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "<class 'spark_rapids_ml.classification.RandomForestClassificationModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 18:57:09 WARN  util.SparkStringUtils: [Thread-6]: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|prediction|fraud_trx|\n",
      "+----------+---------+\n",
      "|       0.0|        0|\n",
      "|       0.0|        0|\n",
      "|       0.0|        0|\n",
      "|       0.0|        0|\n",
      "|       0.0|        1|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "GpuColumnarToRow (8)\n",
      "+- GpuProject (7)\n",
      "   +- GpuCoalesceBatches (6)\n",
      "      +- GpuArrowEvalPython (5)\n",
      "         +- GpuCoalesceBatches (4)\n",
      "            +- GpuSample (3)\n",
      "               +- GpuSort (2)\n",
      "                  +- GpuScan parquet spark_catalog.default.datalaketable (1)\n",
      "\n",
      "\n",
      "(1) GpuScan parquet spark_catalog.default.datalaketable\n",
      "Output [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Batched: true\n",
      "Eager_IO_Prefetch: false\n",
      "Location: InMemoryFileIndex [s3a://go01-demo/warehouse/tablespace/external/hive/customer_data HB01/warehouse/tablespace/external/hive/datalaketable]\n",
      "ReadSchema: struct<age:float,credit_card_balance:float,bank_account_balance:float,mortgage_balance:float,sec_bank_account_balance:float,savings_account_balance:float,sec_savings_account_balance:float,total_est_nworth:float,primary_loan_balance:float,secondary_loan_balance:float,uni_loan_balance:float,longitude:float,latitude:float,transaction_amount:float,fraud_trx:int>\n",
      "\n",
      "(2) GpuSort\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: [age#0 ASC NULLS FIRST, credit_card_balance#1 ASC NULLS FIRST, bank_account_balance#2 ASC NULLS FIRST, mortgage_balance#3 ASC NULLS FIRST, sec_bank_account_balance#4 ASC NULLS FIRST, savings_account_balance#5 ASC NULLS FIRST, sec_savings_account_balance#6 ASC NULLS FIRST, total_est_nworth#7 ASC NULLS FIRST, primary_loan_balance#8 ASC NULLS FIRST, secondary_loan_balance#9 ASC NULLS FIRST, uni_loan_balance#10 ASC NULLS FIRST, longitude#11 ASC NULLS FIRST, latitude#12 ASC NULLS FIRST, transaction_amount#13 ASC NULLS FIRST, fraud_trx#14 ASC NULLS FIRST], false, com.nvidia.spark.rapids.OutOfCoreSort$@5cbdcc5b\n",
      "\n",
      "(3) GpuSample\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: 0.8, 1.0, false, 1234\n",
      "\n",
      "(4) GpuCoalesceBatches\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: targetsize(1073741824)\n",
      "\n",
      "(5) GpuArrowEvalPython\n",
      "Input [15]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14]\n",
      "Arguments: [predict_udf(named_struct(age, age#0, credit_card_balance, credit_card_balance#1, bank_account_balance, bank_account_balance#2, mortgage_balance, mortgage_balance#3, sec_bank_account_balance, sec_bank_account_balance#4, savings_account_balance, savings_account_balance#5, sec_savings_account_balance, sec_savings_account_balance#6, total_est_nworth, total_est_nworth#7, primary_loan_balance, primary_loan_balance#8, secondary_loan_balance, secondary_loan_balance#9, uni_loan_balance, uni_loan_balance#10, longitude, longitude#11, ... 4 more fields))], [pythonUDF0#220], 204\n",
      "\n",
      "(6) GpuCoalesceBatches\n",
      "Input [16]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14, pythonUDF0#220]\n",
      "Arguments: targetsize(1073741824)\n",
      "\n",
      "(7) GpuProject\n",
      "Input [16]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14, pythonUDF0#220]\n",
      "Arguments: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14, pythonUDF0#220.prediction AS prediction#173], true\n",
      "\n",
      "(8) GpuColumnarToRow\n",
      "Input [16]: [age#0, credit_card_balance#1, bank_account_balance#2, mortgage_balance#3, sec_bank_account_balance#4, savings_account_balance#5, sec_savings_account_balance#6, total_est_nworth#7, primary_loan_balance#8, secondary_loan_balance#9, uni_loan_balance#10, longitude#11, latitude#12, transaction_amount#13, fraud_trx#14, prediction#173]\n",
      "Arguments: false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# This runs the training logic in C++ on the GPU via cuML\n",
    "print(\"Training Spark RAPIDS ML model...\")\n",
    "rf_model = rf_classifier.fit(training_data)\n",
    "print(\"Model training complete.\")\n",
    "print(type(rf_model))\n",
    "\n",
    "# Predict and optimize the output\n",
    "# We drop 'probability' and 'rawPrediction' because they are VectorUDT types\n",
    "# that Spark SQL would otherwise force back to the CPU for formatting.\n",
    "predictions = rf_model.transform(test_data).drop(\"probability\", \"rawPrediction\")\n",
    "rf_model.setFeaturesCols(feature_cols)\n",
    "# Show results (This will be fully accelerated)\n",
    "predictions.select(\"prediction\", \"fraud_trx\").show(5)\n",
    "# Verify GPU Plan\n",
    "# You should see 'GpuProject' and 'GpuFilter' nodes without the VectorUDT warning\n",
    "predictions.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7948bbc4-00f1-48ab-96a7-81d1610ed503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU-Accelerated Accuracy: 0.8999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 18:57:21 WARN  rapids.GpuOverrides: [Thread-6]: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, prediction#173, true, false, true), staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, fraud_trx#272, true, false, true), staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, 1.0#273, true, false, true), StructField(prediction,DoubleType,true), StructField(fraud_trx,DoubleType,true), StructField(1.0,DoubleType,false)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    !Expression <StaticInvoke> staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, prediction#173, true, false, true) cannot run on GPU because StaticInvoke of java.lang.Double is not supported on GPU\n",
      "      @Expression <AttributeReference> prediction#173 could run on GPU\n",
      "    !Expression <StaticInvoke> staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, fraud_trx#272, true, false, true) cannot run on GPU because StaticInvoke of java.lang.Double is not supported on GPU\n",
      "      @Expression <AttributeReference> fraud_trx#272 could run on GPU\n",
      "    !Expression <StaticInvoke> staticinvoke(class java.lang.Double, ObjectType(class java.lang.Double), valueOf, 1.0#273, true, false, true) cannot run on GPU because StaticInvoke of java.lang.Double is not supported on GPU\n",
      "      @Expression <AttributeReference> 1.0#273 could run on GPU\n",
      "  !Expression <AttributeReference> obj#277 cannot run on GPU because expression AttributeReference obj#277 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "import spark_rapids_ml.metrics.MulticlassMetrics as mm\n",
    "#print(f\"Available in metrics: {help(mm)}\")\n",
    "\n",
    "\n",
    "accuracy = predictions.filter(\"prediction = fraud_trx\").count() / predictions.count()\n",
    "\n",
    "print(f\"GPU-Accelerated Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from spark_rapids_ml.metrics.MulticlassMetrics import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"fraud_trx\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f987c26c-4d80-420d-9986-68a825adc5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treelite: 4.4.1\n",
      "cuML:     25.10.00\n",
      "spark_rapids_ml:     25.10.0\n"
     ]
    }
   ],
   "source": [
    "treelite_model_checkpoint = rf_model._treelite_model\n",
    "\n",
    "import treelite\n",
    "import cuml\n",
    "import spark_rapids_ml\n",
    "print(f\"Treelite: {treelite.__version__}\")\n",
    "print(f\"cuML:     {cuml.__version__}\")\n",
    "print(f\"spark_rapids_ml:     {spark_rapids_ml.__version__}\")\n",
    "\n",
    "import base64\n",
    "import pickle\n",
    "import treelite\n",
    "\n",
    "\n",
    "# 2. Decode from Base64 to Pickle-bytes\n",
    "pickle_bytes = base64.b64decode(treelite_model_checkpoint)\n",
    "\n",
    "# 3. Unpickle to get the actual Treelite binary bytes\n",
    "# Note: This returns the raw bytes that Treelite actually understands\n",
    "raw_treelite_bytes = pickle.loads(pickle_bytes)\n",
    "\n",
    "# 4. Now deserialize using Treelite\n",
    "treelite_model = treelite.Model.deserialize_bytes(raw_treelite_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a926f1-f3bb-4a35-b4df-fb1b4d541647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Model loaded with 20 trees.\n"
     ]
    }
   ],
   "source": [
    "from cuml.fil import ForestInference\n",
    "\n",
    "fm = ForestInference.load_from_treelite_model(treelite_model)\n",
    "\n",
    "print(f\"Success! Model loaded with {treelite_model.num_tree} trees.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "993dd947-fbff-4b50-93e5-f8e10a9615ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix dimensions: (1979, 15)\n",
      "CPU times: user 0 ns, sys: 8.54 ms, total: 8.54 ms\n",
      "Wall time: 8.53 ms\n"
     ]
    }
   ],
   "source": [
    "test_data_vals=np.array(test_data.collect())\n",
    "print(f\"Input matrix dimensions (rows, cols): {test_data_vals.shape}\")\n",
    "\n",
    "%time FILpredictions = fm.predict(test_data_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c5deb25-4b58-419e-9ee2-d11ad88893e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input matrix dimensions: (19790, 15)\n",
      "CPU times: user 3.11 ms, sys: 1.27 ms, total: 4.38 ms\n",
      "Wall time: 3.74 ms\n",
      "(197900, 15)\n",
      "CPU times: user 0 ns, sys: 19.9 ms, total: 19.9 ms\n",
      "Wall time: 19.9 ms\n",
      "(1979000, 15)\n",
      "CPU times: user 25 ms, sys: 109 ms, total: 134 ms\n",
      "Wall time: 134 ms\n",
      "(3958000, 15)\n",
      "CPU times: user 79.6 ms, sys: 181 ms, total: 261 ms\n",
      "Wall time: 259 ms\n",
      "(7916000, 15)\n",
      "CPU times: user 221 ms, sys: 292 ms, total: 514 ms\n",
      "Wall time: 513 ms\n",
      "(15832000, 15)\n",
      "CPU times: user 364 ms, sys: 661 ms, total: 1.02 s\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "taller_arr = np.tile(test_data_vals, (10, 1))\n",
    "print(f\"input matrix dimensions (rows, cols): {taller_arr.shape}\")\n",
    "%time FILpredictions = fm.predict(taller_arr)\n",
    "\n",
    "taller_arr = np.tile(taller_arr, (10, 1))\n",
    "print(f\"input matrix dimensions (rows, cols): {taller_arr.shape}\")\n",
    "%time FILpredictions = fm.predict(taller_arr)\n",
    "\n",
    "taller_arr = np.tile(taller_arr, (10, 1))\n",
    "print(f\"input matrix dimensions (rows, cols): {taller_arr.shape}\")\n",
    "%time FILpredictions = fm.predict(taller_arr)\n",
    "\n",
    "taller_arr = np.tile(taller_arr, (2, 1))\n",
    "print(f\"input matrix dimensions (rows, cols): {taller_arr.shape}\")\n",
    "%time FILpredictions = fm.predict(taller_arr)\n",
    "\n",
    "taller_arr = np.tile(taller_arr, (2, 1))\n",
    "print(f\"input matrix dimensions (rows, cols): {taller_arr.shape}\")\n",
    "%time FILpredictions = fm.predict(taller_arr)\n",
    "\n",
    "taller_arr = np.tile(taller_arr, (2, 1))\n",
    "print(f\"input matrix dimensions (rows, cols): {taller_arr.shape}\")\n",
    "%time FILpredictions = fm.predict(taller_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1515b4d-d767-48a0-ba52-b92f65b79dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "class FILWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        This runs ONCE when the model is loaded in the registry/production.\n",
    "        It pulls the raw bytes and initializes the GPU model.\n",
    "        \"\"\"\n",
    "        # Load the bytes we saved as an artifact\n",
    "        with open(context.artifacts[\"model_payload\"], \"rb\") as f:\n",
    "            model_bytes = f.read()\n",
    "        \n",
    "        # Initialize the FIL model on the GPU\n",
    "        self.model = ForestInference()\n",
    "        self.model.load_from_treelite_model(model_bytes)\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Standard MLflow predict interface.\n",
    "        \"\"\"\n",
    "        # Ensure input is a GPU-friendly format if possible, \n",
    "        # but ForestInference handles various types.\n",
    "        return self.model.predict(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15319d13-9a32-48d6-8a96-2f8722f11b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'treelite.model.Model'>\n"
     ]
    }
   ],
   "source": [
    "print(type(treelite_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec8de00-8d65-4efa-b951-711c10c5b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bytes = treelite_model.serialize(\"checkpoint.tl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50903448-c7ac-4b96-acee-481e07dcb25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating run for experiment_id: 0, user_id: cdsw, run_name: None\n",
      "No experiment set using default experiment.Please set experiment using mlflow.set_experiment('<your experiment name>') to avoid using default experiment.\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 1832.37it/s] \n",
      "\u001b[31m2026/02/13 15:50:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Successfully registered model 'GPU_RandomForest_Production'.\n",
      "experiment id naci-tqey-gag9-b47c \n",
      "2026/02/13 15:50:07 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: GPU_RandomForest_Production, version 1\n",
      "Created version '1' of model 'GPU_RandomForest_Production'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Define the artifacts for MLflow\n",
    "artifacts = {\"model_payload\": \"checkpoint.tl\"}\n",
    "\n",
    "# 3. Log and Register\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"cuml_fil_model\",\n",
    "        python_model=FILWrapper(),\n",
    "        artifacts=artifacts,\n",
    "        registered_model_name=\"GPU_RandomForest_Production\",\n",
    "        pip_requirements=[\"cuml\", \"treelite\", \"cupy\"] # Vital for the environment!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34101783-9d4c-4987-8af1-31c160404c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
